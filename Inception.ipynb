{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f13370b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\"Hayvan Tanıma Git Repostory Oluşturuldu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a60425",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, History\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # UYARILARI KAPATMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4654 images belonging to 37 classes.\n",
      "Found 1959 images belonging to 37 classes.\n",
      "Found 777 images belonging to 37 classes.\n"
     ]
    }
   ],
   "source": [
    "#print(os.listdir(\"./images/train\"))\n",
    "SIZE = 224\n",
    "EPOCH = 50\n",
    "BATCH = 32\n",
    "train_directory = \"images/train/\"\n",
    "valid_directory = \"images/valid/\"\n",
    "test_directory = \"images/test/\"\n",
    "\n",
    "data_aug = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                              validation_split=0.3\n",
    "                              )\n",
    "test_aug = ImageDataGenerator(rotation_range=40,\n",
    "                              width_shift_range=[0.2, 1.0],\n",
    "                              height_shift_range=[0.2, 1.0],\n",
    "                              shear_range=0.2,\n",
    "                              rescale=1. / 255,\n",
    "                              zoom_range=[0.2, 1.0],\n",
    "                              horizontal_flip=True,\n",
    "                              fill_mode=\"nearest\",\n",
    "                              vertical_flip=True,\n",
    "                              brightness_range=[0.2, 1.0])\n",
    "#Max batch size= available GPU memory bytes / 4 / (size of tensors + trainable parameters)\n",
    "train_generator = data_aug.flow_from_directory(directory=train_directory,\n",
    "                                               batch_size=BATCH,\n",
    "                                               shuffle=True,\n",
    "                                               class_mode=\"categorical\",\n",
    "                                               color_mode=\"rgb\",\n",
    "                                               target_size=(SIZE, SIZE),\n",
    "                                               interpolation=\"nearest\",\n",
    "                                               subset=\"training\"\n",
    "\n",
    "                                               )\n",
    "validation_generator = data_aug.flow_from_directory(directory=train_directory,\n",
    "                                                    batch_size=BATCH,\n",
    "                                                    shuffle=True,\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    target_size=(SIZE, SIZE),\n",
    "                                                    interpolation=\"nearest\",\n",
    "                                                    subset=\"validation\"\n",
    "                                                    )\n",
    "test_generator = data_aug.flow_from_directory(directory=test_directory,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False,\n",
    "                                              class_mode=\"categorical\",\n",
    "                                              target_size=(SIZE, SIZE)\n",
    "                                              )\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Abyssinian': 0, 'Bengal': 1, 'Birman': 2, 'Bombay': 3, 'British_Shorthair': 4, 'Egyptian_Mau': 5, 'Maine_Coon': 6, 'Persian': 7, 'Ragdoll': 8, 'Russian_Blue': 9, 'Siamese': 10, 'Sphynx': 11, 'american_bulldog': 12, 'american_pit_bull_terrier': 13, 'basset_hound': 14, 'beagle': 15, 'boxer': 16, 'chihuahua': 17, 'english_cocker_spaniel': 18, 'english_setter': 19, 'german_shorthaired': 20, 'great_pyrenees': 21, 'havanese': 22, 'japanese_chin': 23, 'keeshond': 24, 'leonberger': 25, 'miniature_pinscher': 26, 'newfoundland': 27, 'pomeranian': 28, 'pug': 29, 'saint_bernard': 30, 'samoyed': 31, 'scottish_terrier': 32, 'shiba_inu': 33, 'staffordshire_bull_terrier': 34, 'wheaten_terrier': 35, 'yorkshire_terrier': 36}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "0 input_1 False\n",
      "1 conv2d False\n",
      "2 batch_normalization False\n",
      "3 activation False\n",
      "4 conv2d_1 False\n",
      "5 batch_normalization_1 False\n",
      "6 activation_1 False\n",
      "7 conv2d_2 False\n",
      "8 batch_normalization_2 False\n",
      "9 activation_2 False\n",
      "10 max_pooling2d False\n",
      "11 conv2d_3 False\n",
      "12 batch_normalization_3 False\n",
      "13 activation_3 False\n",
      "14 conv2d_4 False\n",
      "15 batch_normalization_4 False\n",
      "16 activation_4 False\n",
      "17 max_pooling2d_1 False\n",
      "18 conv2d_8 False\n",
      "19 batch_normalization_8 False\n",
      "20 activation_8 False\n",
      "21 conv2d_6 False\n",
      "22 conv2d_9 False\n",
      "23 batch_normalization_6 False\n",
      "24 batch_normalization_9 False\n",
      "25 activation_6 False\n",
      "26 activation_9 False\n",
      "27 average_pooling2d False\n",
      "28 conv2d_5 False\n",
      "29 conv2d_7 False\n",
      "30 conv2d_10 False\n",
      "31 conv2d_11 False\n",
      "32 batch_normalization_5 False\n",
      "33 batch_normalization_7 False\n",
      "34 batch_normalization_10 False\n",
      "35 batch_normalization_11 False\n",
      "36 activation_5 False\n",
      "37 activation_7 False\n",
      "38 activation_10 False\n",
      "39 activation_11 False\n",
      "40 mixed0 False\n",
      "41 conv2d_15 False\n",
      "42 batch_normalization_15 False\n",
      "43 activation_15 False\n",
      "44 conv2d_13 False\n",
      "45 conv2d_16 False\n",
      "46 batch_normalization_13 False\n",
      "47 batch_normalization_16 False\n",
      "48 activation_13 False\n",
      "49 activation_16 False\n",
      "50 average_pooling2d_1 False\n",
      "51 conv2d_12 False\n",
      "52 conv2d_14 False\n",
      "53 conv2d_17 False\n",
      "54 conv2d_18 False\n",
      "55 batch_normalization_12 False\n",
      "56 batch_normalization_14 False\n",
      "57 batch_normalization_17 False\n",
      "58 batch_normalization_18 False\n",
      "59 activation_12 False\n",
      "60 activation_14 False\n",
      "61 activation_17 False\n",
      "62 activation_18 False\n",
      "63 mixed1 False\n",
      "64 conv2d_22 False\n",
      "65 batch_normalization_22 False\n",
      "66 activation_22 False\n",
      "67 conv2d_20 False\n",
      "68 conv2d_23 False\n",
      "69 batch_normalization_20 False\n",
      "70 batch_normalization_23 False\n",
      "71 activation_20 False\n",
      "72 activation_23 False\n",
      "73 average_pooling2d_2 False\n",
      "74 conv2d_19 False\n",
      "75 conv2d_21 False\n",
      "76 conv2d_24 False\n",
      "77 conv2d_25 False\n",
      "78 batch_normalization_19 False\n",
      "79 batch_normalization_21 False\n",
      "80 batch_normalization_24 False\n",
      "81 batch_normalization_25 False\n",
      "82 activation_19 False\n",
      "83 activation_21 False\n",
      "84 activation_24 False\n",
      "85 activation_25 False\n",
      "86 mixed2 False\n",
      "87 conv2d_27 False\n",
      "88 batch_normalization_27 False\n",
      "89 activation_27 False\n",
      "90 conv2d_28 False\n",
      "91 batch_normalization_28 False\n",
      "92 activation_28 False\n",
      "93 conv2d_26 False\n",
      "94 conv2d_29 False\n",
      "95 batch_normalization_26 False\n",
      "96 batch_normalization_29 False\n",
      "97 activation_26 False\n",
      "98 activation_29 False\n",
      "99 max_pooling2d_2 False\n",
      "100 mixed3 False\n",
      "101 conv2d_34 False\n",
      "102 batch_normalization_34 False\n",
      "103 activation_34 False\n",
      "104 conv2d_35 False\n",
      "105 batch_normalization_35 False\n",
      "106 activation_35 False\n",
      "107 conv2d_31 False\n",
      "108 conv2d_36 False\n",
      "109 batch_normalization_31 False\n",
      "110 batch_normalization_36 False\n",
      "111 activation_31 False\n",
      "112 activation_36 False\n",
      "113 conv2d_32 False\n",
      "114 conv2d_37 False\n",
      "115 batch_normalization_32 False\n",
      "116 batch_normalization_37 False\n",
      "117 activation_32 False\n",
      "118 activation_37 False\n",
      "119 average_pooling2d_3 False\n",
      "120 conv2d_30 False\n",
      "121 conv2d_33 False\n",
      "122 conv2d_38 False\n",
      "123 conv2d_39 False\n",
      "124 batch_normalization_30 False\n",
      "125 batch_normalization_33 False\n",
      "126 batch_normalization_38 False\n",
      "127 batch_normalization_39 False\n",
      "128 activation_30 False\n",
      "129 activation_33 False\n",
      "130 activation_38 False\n",
      "131 activation_39 False\n",
      "132 mixed4 False\n",
      "133 conv2d_44 False\n",
      "134 batch_normalization_44 False\n",
      "135 activation_44 False\n",
      "136 conv2d_45 False\n",
      "137 batch_normalization_45 False\n",
      "138 activation_45 False\n",
      "139 conv2d_41 False\n",
      "140 conv2d_46 False\n",
      "141 batch_normalization_41 False\n",
      "142 batch_normalization_46 False\n",
      "143 activation_41 False\n",
      "144 activation_46 False\n",
      "145 conv2d_42 False\n",
      "146 conv2d_47 False\n",
      "147 batch_normalization_42 False\n",
      "148 batch_normalization_47 False\n",
      "149 activation_42 False\n",
      "150 activation_47 False\n",
      "151 average_pooling2d_4 False\n",
      "152 conv2d_40 False\n",
      "153 conv2d_43 False\n",
      "154 conv2d_48 False\n",
      "155 conv2d_49 False\n",
      "156 batch_normalization_40 False\n",
      "157 batch_normalization_43 False\n",
      "158 batch_normalization_48 False\n",
      "159 batch_normalization_49 False\n",
      "160 activation_40 False\n",
      "161 activation_43 False\n",
      "162 activation_48 False\n",
      "163 activation_49 False\n",
      "164 mixed5 False\n",
      "165 conv2d_54 False\n",
      "166 batch_normalization_54 False\n",
      "167 activation_54 False\n",
      "168 conv2d_55 False\n",
      "169 batch_normalization_55 False\n",
      "170 activation_55 False\n",
      "171 conv2d_51 False\n",
      "172 conv2d_56 False\n",
      "173 batch_normalization_51 False\n",
      "174 batch_normalization_56 False\n",
      "175 activation_51 False\n",
      "176 activation_56 False\n",
      "177 conv2d_52 False\n",
      "178 conv2d_57 False\n",
      "179 batch_normalization_52 False\n",
      "180 batch_normalization_57 False\n",
      "181 activation_52 False\n",
      "182 activation_57 False\n",
      "183 average_pooling2d_5 False\n",
      "184 conv2d_50 False\n",
      "185 conv2d_53 False\n",
      "186 conv2d_58 False\n",
      "187 conv2d_59 False\n",
      "188 batch_normalization_50 False\n",
      "189 batch_normalization_53 False\n",
      "190 batch_normalization_58 False\n",
      "191 batch_normalization_59 False\n",
      "192 activation_50 False\n",
      "193 activation_53 False\n",
      "194 activation_58 False\n",
      "195 activation_59 False\n",
      "196 mixed6 False\n",
      "197 conv2d_64 False\n",
      "198 batch_normalization_64 False\n",
      "199 activation_64 False\n",
      "200 conv2d_65 False\n",
      "201 batch_normalization_65 False\n",
      "202 activation_65 False\n",
      "203 conv2d_61 False\n",
      "204 conv2d_66 False\n",
      "205 batch_normalization_61 False\n",
      "206 batch_normalization_66 False\n",
      "207 activation_61 False\n",
      "208 activation_66 False\n",
      "209 conv2d_62 False\n",
      "210 conv2d_67 False\n",
      "211 batch_normalization_62 False\n",
      "212 batch_normalization_67 False\n",
      "213 activation_62 False\n",
      "214 activation_67 False\n",
      "215 average_pooling2d_6 False\n",
      "216 conv2d_60 False\n",
      "217 conv2d_63 False\n",
      "218 conv2d_68 False\n",
      "219 conv2d_69 False\n",
      "220 batch_normalization_60 False\n",
      "221 batch_normalization_63 False\n",
      "222 batch_normalization_68 False\n",
      "223 batch_normalization_69 False\n",
      "224 activation_60 False\n",
      "225 activation_63 False\n",
      "226 activation_68 False\n",
      "227 activation_69 False\n",
      "228 mixed7 False\n",
      "229 conv2d_72 False\n",
      "230 batch_normalization_72 True\n",
      "231 activation_72 True\n",
      "232 conv2d_73 True\n",
      "233 batch_normalization_73 True\n",
      "234 activation_73 True\n",
      "235 conv2d_70 True\n",
      "236 conv2d_74 True\n",
      "237 batch_normalization_70 True\n",
      "238 batch_normalization_74 True\n",
      "239 activation_70 True\n",
      "240 activation_74 True\n",
      "241 conv2d_71 True\n",
      "242 conv2d_75 True\n",
      "243 batch_normalization_71 True\n",
      "244 batch_normalization_75 True\n",
      "245 activation_71 True\n",
      "246 activation_75 True\n",
      "247 max_pooling2d_3 True\n",
      "248 mixed8 True\n",
      "249 conv2d_80 True\n",
      "250 batch_normalization_80 True\n",
      "251 activation_80 True\n",
      "252 conv2d_77 True\n",
      "253 conv2d_81 True\n",
      "254 batch_normalization_77 True\n",
      "255 batch_normalization_81 True\n",
      "256 activation_77 True\n",
      "257 activation_81 True\n",
      "258 conv2d_78 True\n",
      "259 conv2d_79 True\n",
      "260 conv2d_82 True\n",
      "261 conv2d_83 True\n",
      "262 average_pooling2d_7 True\n",
      "263 conv2d_76 True\n",
      "264 batch_normalization_78 True\n",
      "265 batch_normalization_79 True\n",
      "266 batch_normalization_82 True\n",
      "267 batch_normalization_83 True\n",
      "268 conv2d_84 True\n",
      "269 batch_normalization_76 True\n",
      "270 activation_78 True\n",
      "271 activation_79 True\n",
      "272 activation_82 True\n",
      "273 activation_83 True\n",
      "274 batch_normalization_84 True\n",
      "275 activation_76 True\n",
      "276 mixed9_0 True\n",
      "277 concatenate True\n",
      "278 activation_84 True\n",
      "279 mixed9 True\n",
      "280 conv2d_89 True\n",
      "281 batch_normalization_89 True\n",
      "282 activation_89 True\n",
      "283 conv2d_86 True\n",
      "284 conv2d_90 True\n",
      "285 batch_normalization_86 True\n",
      "286 batch_normalization_90 True\n",
      "287 activation_86 True\n",
      "288 activation_90 True\n",
      "289 conv2d_87 True\n",
      "290 conv2d_88 True\n",
      "291 conv2d_91 True\n",
      "292 conv2d_92 True\n",
      "293 average_pooling2d_8 True\n",
      "294 conv2d_85 True\n",
      "295 batch_normalization_87 True\n",
      "296 batch_normalization_88 True\n",
      "297 batch_normalization_91 True\n",
      "298 batch_normalization_92 True\n",
      "299 conv2d_93 True\n",
      "300 batch_normalization_85 True\n",
      "301 activation_87 True\n",
      "302 activation_88 True\n",
      "303 activation_91 True\n",
      "304 activation_92 True\n",
      "305 batch_normalization_93 True\n",
      "306 activation_85 True\n",
      "307 mixed9_1 True\n",
      "308 concatenate_1 True\n",
      "309 activation_93 True\n",
      "310 mixed10 True\n"
     ]
    }
   ],
   "source": [
    "InceptionV3_model = InceptionV3(input_shape=(SIZE, SIZE, 3), weights='imagenet', include_top=False)\n",
    "for layer in InceptionV3_model.layers[:230]:\n",
    "    layer.trainable = False\n",
    "for layer in InceptionV3_model.layers[230:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "Inception = tf.keras.Sequential([\n",
    "    InceptionV3_model,\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(37, activation='softmax'),\n",
    "\n",
    "])\n",
    "Inception.compile(optimizer=SGD(lr=0.0005, momentum=0.9), loss='categorical_crossentropy',\n",
    "                  metrics=['acc']\n",
    "                  )\n",
    "\n",
    "inception_filepath = 'inceptionv3_' + '-saved-model-{epoch:02d}-loss-{loss:.2f}.hdf5'\n",
    "\n",
    "\n",
    "for i, layer in enumerate(InceptionV3_model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 61\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=inception_filepath,\n",
    "                             save_best_only=True,\n",
    "                             monitor=\"val_acc\",\n",
    "                             mode=\"max\",\n",
    "                             verbose=1\n",
    "                             )\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "history_callback = History()\n",
    "\n",
    "Step_size_train = train_generator.n // train_generator.batch_size\n",
    "Step_size_valid = validation_generator.n // validation_generator.batch_size\n",
    "Step_size_test = test_generator.n // test_generator.batch_size\n",
    "print(Step_size_train, Step_size_valid)\n",
    "\n",
    "inceptionv3_history = Inception.fit(train_generator, epochs=EPOCH,\n",
    "                                    validation_data=validation_generator,\n",
    "                                    callbacks=[history_callback,\n",
    "                                               checkpoint,\n",
    "                                               tensorboard, early],\n",
    "                                    verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(history_callback.history[\"acc\"])\n",
    "plt.plot(history_callback.history['val_acc'])\n",
    "plt.legend(['accuracy', 'validation accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(history_callback.history[\"loss\"])\n",
    "plt.plot(history_callback.history['val_loss'])\n",
    "plt.legend(['accuracy', 'validation accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score, acc = Inception.evaluate_generator(generator=validation_generator,\n",
    "                                                    steps=Step_size_valid)\n",
    "\n",
    "test_generator.reset()\n",
    "\n",
    "pred = Inception.predict_generator(generator=test_generator,\n",
    "                                   steps=Step_size_test,\n",
    "                                   verbose=1)\n",
    "predicted_class_indces = np.argmax(pred, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = train_generator.class_indices\n",
    "labels = dict((v, k) for k, v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indces]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "results = pd.DataFrame({\"Filename\": filenames,\n",
    "                        \"Predictions\": predictions})\n",
    "results.to_csv(\"results.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Test score \", score)\n",
    "print(\"Test acc\", acc)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ufuk/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model = keras.models.load_model(filepath=\"inceptionv3_-saved-model-24-loss-0.01.hdf5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 5, 5, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              52429824  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 37)                37925     \n",
      "=================================================================\n",
      "Total params: 74,270,533\n",
      "Trainable params: 65,132,197\n",
      "Non-trainable params: 9,138,336\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9976\n",
      "Epoch 00001: val_acc improved from -inf to 0.86932, saving model to inceptionv3_-saved-model-01-loss-0.01.hdf5\n",
      "146/146 [==============================] - 115s 791ms/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.5371 - val_acc: 0.8693\n",
      "Epoch 2/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9976\n",
      "Epoch 00002: val_acc improved from 0.86932 to 0.87187, saving model to inceptionv3_-saved-model-02-loss-0.01.hdf5\n",
      "146/146 [==============================] - 95s 650ms/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.5352 - val_acc: 0.8719\n",
      "Epoch 3/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9983\n",
      "Epoch 00003: val_acc did not improve from 0.87187\n",
      "146/146 [==============================] - 93s 636ms/step - loss: 0.0102 - acc: 0.9983 - val_loss: 0.5472 - val_acc: 0.8703\n",
      "Epoch 4/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9972\n",
      "Epoch 00004: val_acc did not improve from 0.87187\n",
      "146/146 [==============================] - 101s 689ms/step - loss: 0.0127 - acc: 0.9972 - val_loss: 0.5691 - val_acc: 0.8668\n",
      "Epoch 5/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9972\n",
      "Epoch 00005: val_acc did not improve from 0.87187\n",
      "146/146 [==============================] - 82s 561ms/step - loss: 0.0105 - acc: 0.9972 - val_loss: 0.5484 - val_acc: 0.8688\n",
      "Epoch 6/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9985\n",
      "Epoch 00006: val_acc did not improve from 0.87187\n",
      "146/146 [==============================] - 98s 671ms/step - loss: 0.0070 - acc: 0.9985 - val_loss: 0.5455 - val_acc: 0.8693\n",
      "Epoch 7/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9974\n",
      "Epoch 00007: val_acc did not improve from 0.87187\n",
      "146/146 [==============================] - 98s 670ms/step - loss: 0.0112 - acc: 0.9974 - val_loss: 0.5562 - val_acc: 0.8673\n",
      "Epoch 8/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9985\n",
      "Epoch 00008: val_acc did not improve from 0.87187\n",
      "146/146 [==============================] - 98s 671ms/step - loss: 0.0079 - acc: 0.9985 - val_loss: 0.5550 - val_acc: 0.8693\n",
      "Epoch 9/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9991\n",
      "Epoch 00009: val_acc did not improve from 0.87187\n",
      "146/146 [==============================] - 100s 683ms/step - loss: 0.0059 - acc: 0.9989 - val_loss: 0.5631 - val_acc: 0.8683\n",
      "Epoch 10/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9981\n",
      "Epoch 00010: val_acc did not improve from 0.87187\n",
      "146/146 [==============================] - 104s 712ms/step - loss: 0.0070 - acc: 0.9981 - val_loss: 0.5613 - val_acc: 0.8683\n",
      "Epoch 11/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9981\n",
      "Epoch 00011: val_acc did not improve from 0.87187\n",
      "146/146 [==============================] - 104s 715ms/step - loss: 0.0083 - acc: 0.9981 - val_loss: 0.5620 - val_acc: 0.8683\n",
      "Epoch 12/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9987\n",
      "Epoch 00012: val_acc improved from 0.87187 to 0.87238, saving model to inceptionv3_-saved-model-12-loss-0.01.hdf5\n",
      "146/146 [==============================] - 110s 756ms/step - loss: 0.0061 - acc: 0.9987 - val_loss: 0.5672 - val_acc: 0.8724\n",
      "Epoch 13/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9978\n",
      "Epoch 00013: val_acc did not improve from 0.87238\n",
      "146/146 [==============================] - 93s 634ms/step - loss: 0.0095 - acc: 0.9979 - val_loss: 0.5730 - val_acc: 0.8617\n",
      "Epoch 14/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9983\n",
      "Epoch 00014: val_acc did not improve from 0.87238\n",
      "146/146 [==============================] - 93s 637ms/step - loss: 0.0082 - acc: 0.9983 - val_loss: 0.6118 - val_acc: 0.8576\n",
      "Epoch 15/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9994\n",
      "Epoch 00015: val_acc did not improve from 0.87238\n",
      "146/146 [==============================] - 87s 599ms/step - loss: 0.0057 - acc: 0.9994 - val_loss: 0.5667 - val_acc: 0.8703\n",
      "Epoch 16/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0102 - acc: 0.9978\n",
      "Epoch 00016: val_acc did not improve from 0.87238\n",
      "146/146 [==============================] - 103s 703ms/step - loss: 0.0102 - acc: 0.9979 - val_loss: 0.6039 - val_acc: 0.8642\n",
      "Epoch 17/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9994\n",
      "Epoch 00017: val_acc did not improve from 0.87238\n",
      "146/146 [==============================] - 95s 648ms/step - loss: 0.0050 - acc: 0.9994 - val_loss: 0.5942 - val_acc: 0.8596\n",
      "Epoch 18/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9983\n",
      "Epoch 00018: val_acc did not improve from 0.87238\n",
      "146/146 [==============================] - 103s 704ms/step - loss: 0.0083 - acc: 0.9983 - val_loss: 0.5763 - val_acc: 0.8632\n",
      "Epoch 19/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9987\n",
      "Epoch 00019: val_acc did not improve from 0.87238\n",
      "146/146 [==============================] - 107s 732ms/step - loss: 0.0058 - acc: 0.9987 - val_loss: 0.5630 - val_acc: 0.8657\n",
      "Epoch 20/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9983\n",
      "Epoch 00020: val_acc did not improve from 0.87238\n",
      "146/146 [==============================] - 86s 589ms/step - loss: 0.0092 - acc: 0.9983 - val_loss: 0.5796 - val_acc: 0.8678\n",
      "Epoch 21/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9985\n",
      "Epoch 00021: val_acc did not improve from 0.87238\n",
      "146/146 [==============================] - 98s 671ms/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.5751 - val_acc: 0.8703\n",
      "Epoch 22/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9991\n",
      "Epoch 00022: val_acc did not improve from 0.87238\n",
      "146/146 [==============================] - 109s 746ms/step - loss: 0.0049 - acc: 0.9991 - val_loss: 0.5697 - val_acc: 0.8632\n",
      "Epoch 23/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 00023: val_acc improved from 0.87238 to 0.87392, saving model to inceptionv3_-saved-model-23-loss-0.00.hdf5\n",
      "146/146 [==============================] - 106s 726ms/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.5484 - val_acc: 0.8739\n",
      "Epoch 24/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9994\n",
      "Epoch 00024: val_acc improved from 0.87392 to 0.87545, saving model to inceptionv3_-saved-model-24-loss-0.00.hdf5\n",
      "146/146 [==============================] - 111s 763ms/step - loss: 0.0047 - acc: 0.9994 - val_loss: 0.5628 - val_acc: 0.8754\n",
      "Epoch 25/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9985\n",
      "Epoch 00025: val_acc did not improve from 0.87545\n",
      "146/146 [==============================] - 81s 554ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.6084 - val_acc: 0.8642\n",
      "Epoch 26/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9989\n",
      "Epoch 00026: val_acc did not improve from 0.87545\n",
      "146/146 [==============================] - 83s 570ms/step - loss: 0.0063 - acc: 0.9989 - val_loss: 0.5678 - val_acc: 0.8734\n",
      "Epoch 27/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n",
      "Epoch 00027: val_acc improved from 0.87545 to 0.88106, saving model to inceptionv3_-saved-model-27-loss-0.00.hdf5\n",
      "146/146 [==============================] - 83s 570ms/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.5416 - val_acc: 0.8811\n",
      "Epoch 28/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 00028: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 85s 579ms/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.5525 - val_acc: 0.8749\n",
      "Epoch 29/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9985\n",
      "Epoch 00029: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 85s 585ms/step - loss: 0.0058 - acc: 0.9985 - val_loss: 0.5472 - val_acc: 0.8749\n",
      "Epoch 30/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9989\n",
      "Epoch 00030: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 135s 924ms/step - loss: 0.0041 - acc: 0.9989 - val_loss: 0.5427 - val_acc: 0.8765\n",
      "Epoch 31/50\n",
      "145/146 [============================>.] - ETA: 1s - loss: 0.0046 - acc: 0.9991\n",
      "Epoch 00031: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 238s 2s/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.5576 - val_acc: 0.8698\n",
      "Epoch 32/50\n",
      "145/146 [============================>.] - ETA: 1s - loss: 0.0039 - acc: 0.9989\n",
      "Epoch 00032: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 244s 2s/step - loss: 0.0039 - acc: 0.9989 - val_loss: 0.5468 - val_acc: 0.8770\n",
      "Epoch 33/50\n",
      "145/146 [============================>.] - ETA: 1s - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 00033: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 290s 2s/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.5718 - val_acc: 0.8724\n",
      "Epoch 34/50\n",
      "145/146 [============================>.] - ETA: 1s - loss: 0.0035 - acc: 0.9994\n",
      "Epoch 00034: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 214s 1s/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.5571 - val_acc: 0.8754\n",
      "Epoch 35/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9985\n",
      "Epoch 00035: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 81s 553ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.5574 - val_acc: 0.8698\n",
      "Epoch 36/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 00036: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 81s 554ms/step - loss: 0.0028 - acc: 0.9996 - val_loss: 0.5537 - val_acc: 0.8709\n",
      "Epoch 37/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 00037: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 81s 552ms/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.5745 - val_acc: 0.8688\n",
      "Epoch 38/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9996\n",
      "Epoch 00038: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 81s 557ms/step - loss: 0.0036 - acc: 0.9996 - val_loss: 0.5871 - val_acc: 0.8673\n",
      "Epoch 39/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9994\n",
      "Epoch 00039: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 81s 553ms/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.5751 - val_acc: 0.8688\n",
      "Epoch 40/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9991\n",
      "Epoch 00040: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 81s 556ms/step - loss: 0.0036 - acc: 0.9991 - val_loss: 0.5974 - val_acc: 0.8673\n",
      "Epoch 41/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9987\n",
      "Epoch 00041: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 81s 557ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.5931 - val_acc: 0.8622\n",
      "Epoch 42/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 00042: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 81s 555ms/step - loss: 0.0024 - acc: 0.9998 - val_loss: 0.6174 - val_acc: 0.8627\n",
      "Epoch 43/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 00043: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 82s 560ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.6094 - val_acc: 0.8617\n",
      "Epoch 44/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 00044: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 84s 576ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.5908 - val_acc: 0.8637\n",
      "Epoch 45/50\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 00045: val_acc did not improve from 0.88106\n",
      "146/146 [==============================] - 86s 591ms/step - loss: 0.0031 - acc: 0.9996 - val_loss: 0.5878 - val_acc: 0.8642\n",
      "Epoch 46/50\n",
      " 12/146 [=>............................] - ETA: 1:00 - loss: 0.0063 - acc: 0.9974"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3868/1758454217.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m                                                \u001B[0mcheckpoint\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                                                tensorboard, early],\n\u001B[0;32m----> 6\u001B[0;31m                                     verbose=1)\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001B[0m\n\u001B[1;32m    671\u001B[0m           \u001B[0muse_multiprocessing\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_multiprocessing\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    672\u001B[0m           \u001B[0mshuffle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshuffle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 673\u001B[0;31m           initial_epoch=initial_epoch)\n\u001B[0m\u001B[1;32m    674\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mtraining_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_eager_dataset_or_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    675\u001B[0m       \u001B[0;31m# Make sure that y, sample_weights, validation_split are not passed.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit_generator\u001B[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001B[0m\n\u001B[1;32m   1431\u001B[0m         \u001B[0mshuffle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshuffle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1432\u001B[0m         \u001B[0minitial_epoch\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minitial_epoch\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1433\u001B[0;31m         steps_name='steps_per_epoch')\n\u001B[0m\u001B[1;32m   1434\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1435\u001B[0m   def evaluate_generator(self,\n",
      "\u001B[0;32m~/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001B[0m in \u001B[0;36mmodel_iteration\u001B[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    263\u001B[0m       \u001B[0mis_deferred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_compiled\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 264\u001B[0;31m       \u001B[0mbatch_outs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mbatch_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    265\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_outs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    266\u001B[0m         \u001B[0mbatch_outs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mbatch_outs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mtrain_on_batch\u001B[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001B[0m\n\u001B[1;32m   1173\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_update_sample_weight_modes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample_weights\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msample_weights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1174\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_train_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1175\u001B[0;31m       \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mins\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1176\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1177\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mreset_metrics\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   3290\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3291\u001B[0m     fetched = self._callable_fn(*array_vals,\n\u001B[0;32m-> 3292\u001B[0;31m                                 run_metadata=self.run_metadata)\n\u001B[0m\u001B[1;32m   3293\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_fetch_callbacks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetched\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3294\u001B[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001B[0;32m~/anaconda3/envs/tensor/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1456\u001B[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001B[1;32m   1457\u001B[0m                                                \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1458\u001B[0;31m                                                run_metadata_ptr)\n\u001B[0m\u001B[1;32m   1459\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1460\u001B[0m           \u001B[0mproto_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#inceptionv3_keep= model.fit(train_generator, epochs=EPOCH, validation_data=validation_generator,   callbacks=[history_callback, checkpoint,tensorboard, early], verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}